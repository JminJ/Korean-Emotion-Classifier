{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "korean_semantic_classification_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTZGQzwSJ2ts"
      },
      "source": [
        "## Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8cGEbp8AVXt",
        "outputId": "f603f6a3-71f0-41a9-cbdd-30c53ce401a8"
      },
      "source": [
        "!pip install import_ipynb"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nk-aGuIKLHn",
        "outputId": "cc0d9c95-a07f-4069-fbc5-0d8afa09a885"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py1KddkcMAHp"
      },
      "source": [
        "import import_ipynb\r\n",
        "import argparse\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import easydict"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frMkqlPSKHxE"
      },
      "source": [
        "import korean_semactic_classification_trainer as T\r\n",
        "import korean_semantic_classification_Dataloader as D\r\n",
        "import korean_semantic_classification_RNN_model as R\r\n",
        "\r\n",
        "# from korean_semactic_classification_trainer.ipynb import Trainer\r\n",
        "# from korean_semantic_classification_Dataloader.ipynb import DataLoader\r\n",
        "# from korean_semantic_classification_RNN_model.ipynb import RNNClassifier"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7salJB0RPzur"
      },
      "source": [
        "## Define out traine code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "eS20kHEMNBXO",
        "outputId": "9777c373-aa92-43ff-f479-d8ce3db80318"
      },
      "source": [
        "def define_argparser(): # colab에서는 easydict를 사용했습니다.\r\n",
        "  args = easydict.EasyDict({\r\n",
        "      'model_fn' : \"\",\r\n",
        "      'train_fn' : '',\r\n",
        "      'gpu' : -1,\r\n",
        "      'verbose' : 2,\r\n",
        "      'min_vocab_freq' : 5,\r\n",
        "      'max_vocab_size' : 999999,\r\n",
        "      'batch_size' : 256,\r\n",
        "      'n_epochs' : 10,\r\n",
        "      'word_vec_size' : 256,\r\n",
        "      'dropout' : .3,\r\n",
        "      'max_length' : 256,\r\n",
        "      'hidden_size' : 512,\r\n",
        "      'n_layers' : 4\r\n",
        "  })\r\n",
        "  config = args\r\n",
        "  return config\r\n",
        "\r\n",
        "def main(config):\r\n",
        "  loaders = D.Dataloader( # error 발생...\r\n",
        "      train_fn = config.train_fn,\r\n",
        "      batch_size = config.batch_size,\r\n",
        "      device = config.gpu,\r\n",
        "      min_freq = config.min_vocab_freq,\r\n",
        "      max_vocab = config.max_vocab_size,\r\n",
        "  )\r\n",
        "\r\n",
        "  print(\r\n",
        "      '|train| = ', len(loaders.train_loader.dataset),\r\n",
        "      '|valid| = ', len(loaders.valid_loaders.dataset),\r\n",
        "  )\r\n",
        "\r\n",
        "  vocab_size = len(loaders.text.vocab)\r\n",
        "  n_classes = len(loaders.label.vocab)\r\n",
        "  print('|vocab| =', vocab_size, '|valid| =', n_classes)\r\n",
        "\r\n",
        "  model = R.rnn_model(\r\n",
        "      input_size = vocab_size,\r\n",
        "      hidden_size = config.hidden_size,\r\n",
        "      word_vec_size = config.word_vec_size,\r\n",
        "      output_size = config.n_classes,\r\n",
        "      n_layers = config.n_layers,\r\n",
        "      dropout_p = config.dropout_p\r\n",
        "  )\r\n",
        "  optimizer = optim.Adam(model.parameters())\r\n",
        "  crit = nn.NLLoss()\r\n",
        "  print(model)\r\n",
        "\r\n",
        "  if config.gpu >= 0:\r\n",
        "    model.cuda(config.gpu)\r\n",
        "    crit.cuda(config.gpu)\r\n",
        "\r\n",
        "  rnn_trainer = T.Trainer(config)\r\n",
        "  rnn_trainer = rnn_trainer.train(\r\n",
        "      model,\r\n",
        "      crit,\r\n",
        "      optimizer,\r\n",
        "      loaders.train_loader,\r\n",
        "      loaders.valid_loader\r\n",
        "  )\r\n",
        "\r\n",
        "  torch.save({\r\n",
        "      'rnn' : rnn_model.state_dict(),\r\n",
        "      'config' : config,\r\n",
        "      'vocab' : loaders.text.vocab,\r\n",
        "      'classes' : loaders.lable.vocab,\r\n",
        "  }, config.model_fn)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "  config = define_argparser()\r\n",
        "  main(config)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-7473fbbdf925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_argparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-7473fbbdf925>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   loaders = D.Dataloader(\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mtrain_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'korean_semantic_classification_Dataloader' has no attribute 'Dataloader'"
          ]
        }
      ]
    }
  ]
}